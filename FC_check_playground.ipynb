{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use : cuda:0\n",
      "sizes\n",
      "104364\n",
      "104364\n",
      "Longer sentence in data : 20\n",
      "cuda:0\n",
      "model import : ./executions/FromGPU4_EncoderUnique/models/Model_Epoch_5.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncoderClassifierDecoder(\n",
       "  (encoder): AttnAutoEncoderRNN(\n",
       "    (embedder): W2VCustomEmbedding(192192, 300)\n",
       "    (encoder): EncoderRNN(\n",
       "      (embedding): W2VCustomEmbedding(192192, 300)\n",
       "      (gru): GRU(300, 512, num_layers=2)\n",
       "    )\n",
       "    (decoder): AttnDecoderRNN(\n",
       "      (embedding): W2VCustomEmbedding(192192, 300)\n",
       "      (attn): Linear(in_features=812, out_features=21, bias=True)\n",
       "      (attn_combine): Linear(in_features=812, out_features=300, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (gru): GRU(300, 512, num_layers=2)\n",
       "      (out): Linear(in_features=512, out_features=192193, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (embedder): W2VCustomEmbedding(192192, 300)\n",
       "  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  (sig_out): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import functions\n",
    "import models\n",
    "import embedder\n",
    "import training_functions\n",
    "from torch.utils import data\n",
    "import glob\n",
    "import dataset\n",
    "from preprocessing import linear_interpolation_collate_fn\n",
    "import time\n",
    "import samplers\n",
    "import frontier\n",
    "import pandas as pd\n",
    "\n",
    "# Set the device parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "print('Device in use : '+str(device))\n",
    "\n",
    "# Create the parameters dict, will be fill after\n",
    "\n",
    "parameters = dict()\n",
    "parameters['device'] = device\n",
    "parameters['tmps_form_last_step'] = time.time()\n",
    "\n",
    "# Should set all parameters of dataloader in this dictionary\n",
    "\n",
    "dataloader_params = dict( # A REVOIR POUR LES DONNEES TWEETS\n",
    "    dataset=None,  # Will change to take dataset\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    batch_sampler=samplers.OppositeSameSizeTwoSentenceBatchSampler,\n",
    "    sampler=None,\n",
    "    num_workers=0,\n",
    "    collate_fn=linear_interpolation_collate_fn,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    timeout=0,\n",
    "    worker_init_fn=None,\n",
    "    divide_by=[1, 2, 5, 20],\n",
    "    divide_at=[0, 20, 30, 50]\n",
    ")\n",
    "\n",
    "# Should set all parameters of criterion in this dictionary\n",
    "\n",
    "embedder_params = dict(\n",
    "    path='./data/model_embedding/fine_tune_W2V.model',\n",
    "    padding_idx=None,\n",
    "    max_norm=None,\n",
    "    norm_type=2.0,\n",
    "    scale_grad_by_freq=False,\n",
    "    sparse=False,\n",
    "    _weight=None\n",
    ")\n",
    "\n",
    "parameters['embedder'] = embedder.W2VCustomEmbedding(**embedder_params).to(parameters['device'])\n",
    "\n",
    "dataloader_params['dataset'] = dataset.YelpTweetDataset(\n",
    "    # path='/home/alexis/Project/Data/NLP_Dataset/all_setences_en_processed.tsv',\n",
    "    path='../Data/Yelp/',\n",
    "    file_name='20review_binary',\n",
    "    file_type='csv',\n",
    "    device=parameters['device'],\n",
    "    return_id=True,\n",
    "    text_column='text',\n",
    "    label_column='target')\n",
    "\n",
    "# Set True or False for padable\n",
    "\n",
    "dataloader_params['dataset'].set_embedder(parameters)\n",
    "\n",
    "parameters['pad_token'] = parameters['embedder'].word2index['<pad>']\n",
    "\n",
    "# Should set all parameters of model in this dictionary\n",
    "\n",
    "'''model_params = dict(\n",
    "    ntoken=len(parameters['embedder'].word2index),  # len(TEXT.vocab.stoi), # the size of vocabulary\n",
    "    ninp=parameters['embedder'].embedding_dim,  # embedding dimension\n",
    "    nhid=512,  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "    nlayers=6,  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder 10-16\n",
    "    nhead=10,  # the number of heads in the multi_head_attention models\n",
    "    dropout=0.1,\n",
    "    device=parameters['device']\n",
    ")'''\n",
    "\n",
    "print('Longer sentence in data : '+str(max(dataloader_params['dataset'].size)))\n",
    "\n",
    "encoder_params = dict(\n",
    "    embedder=parameters['embedder'],\n",
    "    dropout_p=0.1,\n",
    "    device=parameters['device'],\n",
    "    teacher_forcing_ratio=0,  # Non entrainement\n",
    "    num_layers=2,\n",
    "    bidirectional=False,\n",
    "    encode_size=512,\n",
    "    max_length=max(dataloader_params['dataset'].size)\n",
    ")\n",
    "\n",
    "# classifier_params = dict(\n",
    "#     embedder=parameters['embedder'],\n",
    "#     dropout=0.5,\n",
    "#     layer_dropout=0.3,\n",
    "#     device=parameters['device'], # a voir si je le laisse\n",
    "#     n_layers=2,\n",
    "#     bidirectional=False,\n",
    "#     n_hidden=512,\n",
    "#     n_out=2 #formule pour récupérer le nombre de classe du dataset\n",
    "# )\n",
    "\n",
    "model_params = dict(\n",
    "    num_class=dataloader_params['dataset'].num_class\n",
    ")\n",
    "\n",
    "parameters['encoder_model'] = models.AttnAutoEncoderRNN(**encoder_params).to(parameters['device'])  #models.TransformerModel(**model_params).to(parameters['device'])\n",
    "# parameters['encoder_model'].load_state_dict(torch.load(str(\"./executions/FromGPU4_MediumFixed/models/Best_Model_Epoch_20.pt\"), map_location=device))\n",
    "# parameters['classifier_model'] = models.SentimentRNN(**classifier_params).to(parameters['device'])  #models.TransformerModel(**model_params).to(parameters['device'])\n",
    "# parameters['model'] = models.EncoderClassifier(parameters['encoder_model'], parameters['classifier_model'], parameters['embedder'])\n",
    "parameters['model'] = models.EncoderClassifierDecoder(parameters['encoder_model'], parameters['embedder'], model_params['num_class'], device)\n",
    "\n",
    "name_execution = 'FromGPU4_EncoderUnique' # A CHANGER\n",
    "\n",
    "#with open(\"./executions/\" + name_execution + \"/model.pkl\", 'rb') as f:\n",
    "    #model = pkl.load(f)\n",
    "parameters['model'] = parameters['model'].to(parameters['device'])  #models.TransformerModel(**model_params).to(parameters['device'])\n",
    "parameters['encoder_model'] = parameters['model'].encoder\n",
    "parameters['classifier_model'] = parameters['model'].classifier\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data)\n",
    "\n",
    "#with open(\"./executions/\" + name_execution + \"/embedder.pkl\", 'rb') as f:\n",
    "    #embedder = pkl.load(f)\n",
    "for f in glob.glob(\"./executions/\" + str(name_execution) + \"/models/Model_Epoch_5.pt\"):\n",
    "    print('model import : '+str(f))\n",
    "    parameters['model'].load_state_dict(torch.load(str(f), map_location=device))\n",
    "# model = torch.load(str(\"executions/FromGPU4_Short/models/Best_Model_Epoch_18.pt\"))\n",
    "parameters['model'].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.weight torch.Size([2, 1024])\n",
      "classifier.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for name, param in parameters['model'].named_parameters():\n",
    "    if param.requires_grad and \"classifier\" in name:\n",
    "        if \"weight\" in name:\n",
    "            weight = param.data.cpu().data.numpy()\n",
    "        else :\n",
    "            biais = param.data.cpu().data.numpy()\n",
    "        print(name, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09971751, -0.07739043, -0.3053918 , ...,  0.01417913,\n",
       "         0.17426673, -0.05969966],\n",
       "       [ 0.02784912,  0.01768062,  0.13319671, ..., -0.0218176 ,\n",
       "        -0.03291409,  0.1052188 ]], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00771963, -0.02887552], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "plans = pd.DataFrame(data=None, index = None, columns=list(range(1024))+['biais']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "      <th>biais</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 1025 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "plans = plans.append(pd.DataFrame(data=[np.append(weight[0], biais[0])], columns=plans.columns))\n",
    "plans = plans.append(pd.DataFrame(data=[np.append(weight[1], biais[1])], columns=plans.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "      <th>biais</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.099718</td>\n",
       "      <td>-0.077390</td>\n",
       "      <td>-0.305392</td>\n",
       "      <td>0.256532</td>\n",
       "      <td>-0.033397</td>\n",
       "      <td>0.220542</td>\n",
       "      <td>0.105602</td>\n",
       "      <td>-0.051412</td>\n",
       "      <td>-0.292714</td>\n",
       "      <td>0.012570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048517</td>\n",
       "      <td>-0.018064</td>\n",
       "      <td>-0.039292</td>\n",
       "      <td>0.071533</td>\n",
       "      <td>-0.090711</td>\n",
       "      <td>-0.501262</td>\n",
       "      <td>0.014179</td>\n",
       "      <td>0.174267</td>\n",
       "      <td>-0.059700</td>\n",
       "      <td>0.007720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.027849</td>\n",
       "      <td>0.017681</td>\n",
       "      <td>0.133197</td>\n",
       "      <td>-0.098239</td>\n",
       "      <td>0.030657</td>\n",
       "      <td>-0.089705</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.136552</td>\n",
       "      <td>0.187572</td>\n",
       "      <td>-0.009032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008740</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.013864</td>\n",
       "      <td>-0.100081</td>\n",
       "      <td>0.069355</td>\n",
       "      <td>0.321744</td>\n",
       "      <td>-0.021818</td>\n",
       "      <td>-0.032914</td>\n",
       "      <td>0.105219</td>\n",
       "      <td>-0.028876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.099718 -0.077390 -0.305392  0.256532 -0.033397  0.220542  0.105602   \n",
       "0  0.027849  0.017681  0.133197 -0.098239  0.030657 -0.089705  0.001464   \n",
       "\n",
       "          7         8         9  ...      1015      1016      1017      1018  \\\n",
       "0 -0.051412 -0.292714  0.012570  ...  0.048517 -0.018064 -0.039292  0.071533   \n",
       "0  0.136552  0.187572 -0.009032  ... -0.008740  0.002646  0.013864 -0.100081   \n",
       "\n",
       "       1019      1020      1021      1022      1023     biais  \n",
       "0 -0.090711 -0.501262  0.014179  0.174267 -0.059700  0.007720  \n",
       "0  0.069355  0.321744 -0.021818 -0.032914  0.105219 -0.028876  \n",
       "\n",
       "[2 rows x 1025 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"last_FC.pkl\", \"wb\") as output_file:\n",
    "    pkl.dump(plans, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
